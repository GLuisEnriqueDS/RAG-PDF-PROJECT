import os
from dotenv import load_dotenv
import google.generativeai as genai

from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import SentenceTransformerEmbeddings
from PyPDF2 import PdfReader 

# ==========================================
# CONFIGURACI√ìN INICIAL
# ==========================================
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError("Falta GEMINI_API_KEY en el archivo .env")
genai.configure(api_key=GEMINI_API_KEY)

PDF_DIR = "data/pdfs"
CHROMA_DIR = "embeddings/chroma_db"
os.makedirs(PDF_DIR, exist_ok=True)
os.makedirs(CHROMA_DIR, exist_ok=True)

# Define la ruta para el √≠ndice de persistencia de FAISS
FAISS_INDEX_PATH = os.path.join(CHROMA_DIR, "faiss_index")

# ==========================================
# CONFIGURACI√ìN DE EMBEDDINGS (Local)
# ==========================================
# Esto reemplaza a LocalEmbeddingFunction
print("Inicializando modelo de embedding local...")
EMBEDDINGS = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
print("Modelo de embedding local cargado.")

# ==========================================
# üìö BASE VECTORIAL FAISS (Todo-en-uno)
# ==========================================

def get_vectorstore():
    # 1. Intentar cargar el √≠ndice existente (Persistencia)
    if os.path.exists(FAISS_INDEX_PATH):
        print("Cargando √≠ndice FAISS existente...")
        # Nota: 'allow_dangerous_deserialization=True' es necesario para LangChain 0.2+
        return FAISS.load_local(FAISS_INDEX_PATH, EMBEDDINGS, allow_dangerous_deserialization=True)
        
    # 2. Si no existe, crear el √≠ndice desde cero
    print("Creando FAISS Vector Store por primera vez...")
    
    docs = []
    pdf_files = [f for f in os.listdir(PDF_DIR) if f.endswith(".pdf")]
    
    # Cargar y dividir en una sola pasada usando LangChain Loaders
    for f in pdf_files:
        path = os.path.join(PDF_DIR, f)
        try:
            # PyPDFLoader maneja la carga de forma m√°s robusta
            loader = PyPDFLoader(path)
            docs.extend(loader.load()) 
        except Exception as e:
            print(f"‚ùå Error al cargar el PDF {f}: {e}")
            continue

    if not docs:
        print("No se encontraron documentos v√°lidos. Devuelve None.")
        return None

    # 3. Dividir el texto (LangChain Text Splitter)
    text_splitter = CharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=200, 
        separator="\n",
        length_function=len,
        is_separator_regex=False
    )
    split_docs = text_splitter.split_documents(docs)
    
    print(f"Total de chunks a procesar: {len(split_docs)}")

    # 4. Crear el √≠ndice FAISS (aqu√≠ se generan los embeddings)
    print("Generando embeddings y creando √≠ndice FAISS...")
    # FAISS.from_documents genera los embeddings y crea el √≠ndice
    vectorstore = FAISS.from_documents(split_docs, EMBEDDINGS)
    
    # 5. Guardar el √≠ndice
    vectorstore.save_local(CHROMA_DIR, index_name="faiss_index")
    print("√çndice FAISS creado y guardado correctamente.")
    
    return vectorstore

# ==========================================
# QUERY RAG
# ==========================================
def query_rag(question, top_k=3):
    vectorstore = get_vectorstore()
    if not vectorstore:
        return "No hay documentos para consultar."

    # 1. Obtener los documentos de contexto relevantes
    # similarity_search devuelve Documentos de LangChain
    results = vectorstore.similarity_search(question, k=top_k)
    
    # 2. Compilar el contexto (tomando el contenido de la p√°gina)
    context_text = "\n\n".join([doc.page_content for doc in results])
    
    # Verificar si la consulta de FAISS tiene al menos 1 resultado
    if not context_text.strip():
        return "No se encontr√≥ informaci√≥n relevante en los documentos para responder a la pregunta."


    prompt = f"""
Eres un asistente que responde preguntas usando √∫nicamente la informaci√≥n de los documentos a continuaci√≥n. Si la informaci√≥n no est√° presente, simplemente indica que no puedes responder bas√°ndote en el contexto.

--- CONTEXTO EXTRA√çDO DE DOCUMENTOS ---
{context_text}
--- FIN DEL CONTEXTO ---

Pregunta: {question}

Respuesta clara y concisa:
"""
    try:
        modelo = genai.GenerativeModel("gemini-2.0-flash")
        respuesta = modelo.generate_content(prompt)
        return respuesta.text
    except Exception as e:
        if "429" in str(e):
            return " El servicio de Gemini est√° temporalmente saturado. Intenta de nuevo en unos minutos."
        else:
            raise e